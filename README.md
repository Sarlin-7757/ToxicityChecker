# Toxicity Checker using LSTM and Jigsaw Toxicity Dataset

👤 Author: Sarlin7757 JailBreakerVC

![image](https://github.com/Sarlin-7757/ToxicityChecker/assets/108054534/05128a8d-1742-487b-9f13-717615fafbab)

## Overview
This repository presents a toxicity checker implemented using Long Short-Term Memory (LSTM) networks, trained on the Jigsaw Toxicity Dataset. The model is designed to detect toxic comments in text data with high accuracy.

## Features
🧠 Utilizes LSTM architecture for sequence modeling
📊 Trained on the Jigsaw Toxicity Dataset for toxicity detection
🔍 Provides a reliable solution for identifying toxic comments in various contexts

## Requirements
- Python 3.11
- TensorFlow
- Jigsaw Toxicity Dataset


## Installation
1. Clone this repository:
   ```bash
   git clone https://github.com/your_username/toxicity-checker.git
   cd toxicity-checker
