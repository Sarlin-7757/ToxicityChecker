# Toxicity Checker using LSTM and Jigsaw Toxicity Dataset

ğŸ‘¤ Author: Sarlin7757 JailBreakerVC

## Overview
This repository presents a toxicity checker implemented using Long Short-Term Memory (LSTM) networks, trained on the Jigsaw Toxicity Dataset. The model is designed to detect toxic comments in text data with high accuracy.

## Features
ğŸ§  Utilizes LSTM architecture for sequence modeling
ğŸ“Š Trained on the Jigsaw Toxicity Dataset for toxicity detection
ğŸ” Provides a reliable solution for identifying toxic comments in various contexts

## Requirements
- Python 3.x
- TensorFlow
- Jigsaw Toxicity Dataset


## Installation
1. Clone this repository:
   ```bash
   git clone https://github.com/your_username/toxicity-checker.git
   cd toxicity-checker
